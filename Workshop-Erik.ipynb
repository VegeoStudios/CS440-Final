{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTES:\n",
    "[GPT-2 Classification Getting Started](https://www.kaggle.com/code/andres6garzon/getting-started-nlp-classification-using-gpt-2)\n",
    "\n",
    "[GPT-2 (Medium)](https://huggingface.co/openai-community/gpt2-medium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Erik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from datasets import Dataset, load_dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import (set_seed,\n",
    "                          TrainingArguments,\n",
    "                          Trainer,\n",
    "                          GPT2Config,\n",
    "                          GPT2Tokenizer,\n",
    "                          AdamW, \n",
    "                          get_linear_schedule_with_warmup,\n",
    "                          GPT2ForSequenceClassification)\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = ['java', 'python', 'pharo']\n",
    "labels = {\n",
    "    'java': ['summary', 'Ownership', 'Expand', 'usage', 'Pointer', 'deprecation', 'rational'],\n",
    "    'python': ['Usage', 'Parameters', 'DevelopmentNotes', 'Expand', 'Summary'],\n",
    "    'pharo': ['Keyimplementationpoints', 'Example', 'Responsibilities', 'Classreferences', 'Intent', 'Keymessages', 'Collaborators']\n",
    "}\n",
    "ds = load_dataset('NLBSE/nlbse25-code-comment-classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'index': 0,\n",
       " 'class': 'Abfss.java',\n",
       " 'comment_sentence': 'azure blob file system implementation of abstractfilesystem.',\n",
       " 'partition': 0,\n",
       " 'combo': 'azure blob file system implementation of abstractfilesystem. | Abfss.java',\n",
       " 'labels': [1, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['java_train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncate_length = 256\n",
    "\n",
    "def preprocess(dataset):\n",
    "    # Create dataset object\n",
    "    output = []\n",
    "    \n",
    "    for i in tqdm(range(len(dataset))):\n",
    "        \n",
    "        text = dataset[i]['combo']\n",
    "        \n",
    "        # remove entirety of html lists\n",
    "        # text = re.sub(r'<ol>[.\\s\\S]*?<\\/ol>', '', text)\n",
    "        \n",
    "        # remove html tags\n",
    "        text = re.sub(r'<.*?>', '', text)\n",
    "        \n",
    "        # remove bullets\n",
    "        text = re.sub(r'\\s\\*', '', text)\n",
    "        \n",
    "        # remove bulleted lines\n",
    "        #text = re.sub(r'\\n\\s*\\*.*', '', text)\n",
    "        \n",
    "        # remove curly braced sections\n",
    "        text = re.sub(r'\\{.*?\\}', '', text)\n",
    "        \n",
    "        # remove // comments\n",
    "        text = re.sub(r'\\s*\\/\\/.*', '', text)\n",
    "        \n",
    "        # remove formatting for /* */ comments\n",
    "        text = re.sub(r'\\/\\*.|\\*\\/', '', text)\n",
    "        \n",
    "        # remove multiple spaces\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        # truncate\n",
    "        if (len(text) > truncate_length * 2):\n",
    "            text = text[:(truncate_length-4)] + ' ... ' + text[-(truncate_length-4):]\n",
    "        \n",
    "        output.append({\n",
    "            'text': text,\n",
    "            'label': np.argmax(dataset[i]['labels'])\n",
    "        })\n",
    "    \n",
    "    output = pd.DataFrame(output)\n",
    "    output = Dataset.from_pandas(output)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0558eefe5a642b4a2a106c6d2d4e3ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7614 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old length: 738\n",
      "New length: 511\n",
      "====================================================================================================\n",
      "A builder for creating immutable bimap instances, especially {@code public\n",
      "   * static final} bimaps (\"constant bimaps\"). Example: <pre>   {@code\n",
      "   *\n",
      "   *   static final ImmutableBiMap<String, Integer> WORD_TO_INT =\n",
      "   *       new ImmutableBiMap.Builder<String, Integer>()\n",
      "   *           .put(\"one\", 1)\n",
      "   *           .put(\"two\", 2)\n",
      "   *           .put(\"three\", 3)\n",
      "   *           .build();}</pre>\n",
      "   *\n",
      "   * <p>For <i>small</i> immutable bimaps, the {@code ImmutableBiMap.of()} methods\n",
      "   * are even more convenient.\n",
      "   *\n",
      "   * <p>Builder instances can be reused - it is safe to call {@link #build}\n",
      "   * multiple times to build multiple bimaps in series. Each bimap is a superset\n",
      "   * of the bimaps created before it. | ImmutableBiMap.java\n",
      "====================================================================================================\n",
      "A builder for creating immutable bimap instances, especially {@code public static final} bimaps (\"constant bimaps\"). Example: {@code static final ImmutableBiMap WORD_TO_INT = new ImmutableBiMap.Builder() .put(\"one\", 1) .put(\"two\", 2) .put(\"three\", 3) .build();} For small immutable bimaps, the methods are even more convenient. Builder instances can be reused - it is safe to call multiple times to build multiple bimaps in series. Each bimap is a superset of the bimaps created before it. | ImmutableBiMap.java\n"
     ]
    }
   ],
   "source": [
    "length = 0\n",
    "index = -1\n",
    "i = 0\n",
    "\n",
    "test_ds = preprocess(ds['java_train'])\n",
    "\n",
    "for data in test_ds:\n",
    "    if len(data['text']) > length:\n",
    "        length = len(data['text'])\n",
    "        index = i\n",
    "    i += 1\n",
    "        \n",
    "print(f\"Old length: {len(ds['java_train'][index]['combo'])}\")\n",
    "print(f\"New length: {length}\")\n",
    "print('='*100)\n",
    "print(ds['java_train'][index]['combo'])\n",
    "print('='*100)\n",
    "print(test_ds[index]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model):\n",
    "    total_flops = 0\n",
    "    total_time = 0\n",
    "    scores = []\n",
    "    for lan in langs:\n",
    "        with torch.profiler.profile(with_flops=True) as p:\n",
    "            begin = time.time()\n",
    "            for i in range(10):\n",
    "                y_pred = model(ds[f'{lan}_test']['combo']).numpy().T\n",
    "            total = time.time() - begin\n",
    "            total_time = total_time + total\n",
    "        total_flops = total_flops + (sum(k.flops for k in p.key_averages()) / 1e9)\n",
    "        y_true = np.array(ds[f'{lan}_test']['labels']).T\n",
    "        for i in range(len(y_pred)):\n",
    "            assert(len(y_pred[i]) == len(y_true[i]))\n",
    "            tp = sum([true == pred == 1 for (true,pred) in zip(y_true[i], y_pred[i])])\n",
    "            tn = sum([true == pred == 0 for (true,pred) in zip(y_true[i], y_pred[i])])\n",
    "            fp = sum([true == 0 and pred == 1 for (true,pred) in zip(y_true[i], y_pred[i])])\n",
    "            fn = sum([true == 1 and pred == 0 for (true,pred) in zip(y_true[i], y_pred[i])])\n",
    "            precision = tp / (tp + fp)\n",
    "            recall = tp / (tp + fn)\n",
    "            f1 = (2*tp) / (2*tp + fp + fn)\n",
    "            scores.append({'lan': lan, 'cat': labels[lan][i],'precision': precision,'recall': recall,'f1': f1})\n",
    "    print(\"Compute in GFLOPs:\", total_flops/10)\n",
    "    print(\"Avg runtime in seconds:\", total_time/10)\n",
    "    scores = pd.DataFrame(scores)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params\n",
    "seed = 42\n",
    "batch_size = 4\n",
    "max_length = 512\n",
    "num_labels = 7\n",
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetCreator(Dataset):\n",
    "    def __init__(self, ds, train):\n",
    "        self.ds = ds\n",
    "        self.train = train\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.train:\n",
    "            return {'text': self.ds[idx]['text'], 'label': self.ds[idx]['label']}\n",
    "        else:\n",
    "            return {'text': self.ds[idx]['text'], 'label': 0}\n",
    "\n",
    "class GPT2_collator(object):\n",
    "    def __init__(self, tokenizer, max_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __call__(self, sequences):\n",
    "        texts = [sequence['text'] for sequence in sequences]\n",
    "        labels = [sequence['label'] for sequence in sequences]\n",
    "        \n",
    "        inputs = self.tokenizer(text=texts, return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_length)\n",
    "        inputs.update({'labels': torch.tensor(labels)})\n",
    "        \n",
    "        return inputs\n",
    "# =================================================================================================================================\n",
    "def train(dataloader, optimizer, scheduler, device, max_batches=None):\n",
    "    global model\n",
    "    model.train()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    total_loss = 0\n",
    "    batch_count = 0\n",
    "    \n",
    "    for batch in tqdm(dataloader, total=len(dataloader)):\n",
    "        batch_true_labels = batch['labels'].numpy().flatten().tolist()\n",
    "        batch = {k:v.to(device) for k,v in batch.items()}\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**batch)\n",
    "        loss, logits = outputs[:2]\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        batch_predictions = logits.argmax(axis=-1).flatten().tolist()\n",
    "        \n",
    "        predictions += batch_predictions\n",
    "        true_labels += batch_true_labels\n",
    "        \n",
    "        batch_count += 1\n",
    "        if max_batches and batch_count >= max_batches:\n",
    "            break\n",
    "    \n",
    "    avg_epoch_loss = total_loss / batch_count\n",
    "    \n",
    "    return predictions, true_labels, avg_epoch_loss\n",
    "\n",
    "def validate(dataloader, device, max_batches=None):\n",
    "    global model\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    total_loss = 0\n",
    "    batch_count = 0\n",
    "    \n",
    "    for batch in tqdm(dataloader, total=len(dataloader)):\n",
    "        batch_true_labels = batch['labels'].numpy().flatten().tolist()\n",
    "        batch = {k:v.to(device) for k,v in batch.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "            loss, logits = outputs[:2]\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        batch_predictions = logits.argmax(axis=-1).flatten().tolist()\n",
    "        \n",
    "        predictions += batch_predictions\n",
    "        true_labels += batch_true_labels\n",
    "        \n",
    "        batch_count += 1\n",
    "        if max_batches and batch_count >= max_batches:\n",
    "            break\n",
    "    \n",
    "    avg_epoch_loss = total_loss / batch_count\n",
    "    \n",
    "    return predictions, true_labels, avg_epoch_loss\n",
    "\n",
    "def predict(dataloader, device):\n",
    "    global model\n",
    "    model.eval()\n",
    "    predictions_labels = []\n",
    "    \n",
    "    for batch in tqdm(dataloader, total=len(dataloader)):\n",
    "        batch = {k:v.type(torch.long).to(device) for k,v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "            logits = outputs[0]\n",
    "            predictions_labels += logits.argmax(axis=-1).flatten().tolist()\n",
    "    return predictions_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting config...\n",
      "Loading tokenizer...\n",
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2ForSequenceClassification(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (score): Linear(in_features=768, out_features=7, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Setting config...')\n",
    "model_config = GPT2Config.from_pretrained('gpt2', num_labels=num_labels)\n",
    "\n",
    "print('Loading tokenizer...')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.padding_side = 'left'\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print('Loading model...')\n",
    "model = GPT2ForSequenceClassification.from_pretrained('gpt2', config=model_config)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b20552168a54a159f222ba72abbe0d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7614 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81c8c9ad1d284b7381ae777126fb9c41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1725 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gpt2_collator = GPT2_collator(tokenizer, max_length=max_length)\n",
    "\n",
    "print('Creating datasets...')\n",
    "\n",
    "train_dataset = DatasetCreator(preprocess(ds['java_train']), True)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=gpt2_collator)\n",
    "\n",
    "eval_dataset = DatasetCreator(preprocess(ds['java_test']), True)\n",
    "eval_dataloader = torch.utils.data.DataLoader(eval_dataset, batch_size=batch_size, shuffle=True, collate_fn=gpt2_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Erik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5b306cf34134b219672f8f0b17e04a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9320cb119b7b48e89e03affb66fe80e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1904 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 - Train Loss: 1.5365825262665749, Train Accuracy: 0.55\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1b95bcf72fc47eea1197ff17af393c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/432 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 3, 3, 0, 3, 3, 3, 0, 0, 3, 3, 0, 3, 3, 3, 0, 3, 3, 0, 3, 0, 3, 0, 3, 0, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 3, 0, 0, 3, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 3, 3, 0, 0, 0, 3, 0, 3, 0, 0, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 3, 3, 0, 0, 3, 0, 3, 0, 3, 3, 0, 3, 0, 3, 0, 3, 0, 0, 0, 3, 0, 3, 0, 0, 0, 3, 0, 3, 0, 0, 0, 3, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 3, 3, 0, 0, 3, 0, 0, 0, 0, 0, 3, 0, 3, 0, 0, 0, 0, 3, 3, 0, 0, 0, 0, 3, 3, 0, 3, 0, 0, 0, 0, 3, 0, 3, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 3, 0, 0, 0, 3, 0, 0, 0, 0, 0, 3, 0, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 0, 3, 3, 0, 0, 0, 0, 3, 0, 3, 3, 3, 0, 3, 0, 0, 3, 3, 0, 0, 0, 3, 3, 3, 0, 0, 0, 3, 0, 3, 0, 0, 3, 3, 0, 0, 0, 0, 3, 0, 0, 3, 0, 3, 0, 0, 0, 3, 3, 0, 0, 0, 3, 0, 3, 0, 0, 0, 0, 3, 3, 0, 0, 0, 3, 0, 0, 3, 0, 0, 0, 0, 3, 0, 3, 3, 3, 0, 3, 0, 0, 0, 3, 3, 0, 0, 3, 3, 3, 0, 0, 0, 0, 3, 3, 0, 3, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 3, 0, 0, 3, 0, 3, 0, 0, 0, 3, 3, 0, 3, 3, 0, 0, 3, 0, 3, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 3, 0, 0, 0, 0, 3, 3, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 0, 0, 0, 0, 0, 0]\n",
      "Epoch 1/1 - Eval Loss: 1.3846751952171326, Eval Accuracy: 0.6725\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr = 5e-5, eps = 1e-8, weight_decay=0.01)\n",
    "total_steps = len(train_dataloader) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "loss = []\n",
    "accuracy = []\n",
    "eval_loss_list = []\n",
    "eval_accuracy_list = []\n",
    "\n",
    "max_batches = 100\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    train_labels, true_labels, train_loss = train(train_dataloader, optimizer, scheduler, device, max_batches=max_batches)\n",
    "    train_acc = accuracy_score(true_labels, train_labels)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss}, Train Accuracy: {train_acc}')\n",
    "    loss.append(train_loss)\n",
    "    accuracy.append(train_acc)\n",
    "    \n",
    "    eval_labels, true_labels, eval_loss = validate(eval_dataloader, device, max_batches=max_batches)\n",
    "    print(eval_labels)\n",
    "    eval_acc = accuracy_score(true_labels, eval_labels)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs} - Eval Loss: {eval_loss}, Eval Accuracy: {eval_acc}')\n",
    "    eval_loss_list.append(eval_loss)\n",
    "    eval_accuracy_list.append(eval_acc)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
